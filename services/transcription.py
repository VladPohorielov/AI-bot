import whisper
import torch
import warnings
import os
import subprocess
import shutil
from config import WHISPER_MODEL_SIZE

warnings.filterwarnings("ignore", message="FP16 is not supported on CPU; using FP32 instead")


def check_ffmpeg():
    """Check if ffmpeg is available"""
    try:
        # Try to find ffmpeg using shutil.which
        ffmpeg_path = shutil.which('ffmpeg')
        if ffmpeg_path:
            print(f"‚úÖ FFmpeg –Ω–∞–π–¥–µ–Ω: {ffmpeg_path}")
            return True
        
        # Try to run ffmpeg command
        result = subprocess.run(['ffmpeg', '-version'], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            print("‚úÖ FFmpeg –¥–æ—Å—Ç—É–ø–µ–Ω")
            return True
        else:
            print("‚ùå FFmpeg –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ PATH")
            return False
    except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError) as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ FFmpeg: {e}")
        return False


def load_whisper_model():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Whisper model loading on: {device}")
    
    # Check ffmpeg availability
    if not check_ffmpeg():
        print("‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: FFmpeg –Ω–µ –Ω–∞–π–¥–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∞—É–¥–∏–æ —Ñ–æ—Ä–º–∞—Ç–æ–≤ –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å.")
        print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ FFmpeg: winget install ffmpeg")
    
    model = whisper.load_model(WHISPER_MODEL_SIZE, device=device)
    print("Whisper model loaded.")
    return model, device


async def transcribe_audio(model, file_path: str, language: str = "auto") -> str:
    try:
        # Check if file exists
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"–ê—É–¥–∏–æ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        
        # Check file size
        file_size = os.path.getsize(file_path)
        if file_size == 0:
            raise ValueError("–ê—É–¥–∏–æ —Ñ–∞–π–ª –ø—É—Å—Ç")
        
        print(f"üéµ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é —Ñ–∞–π–ª: {file_path} (—Ä–∞–∑–º–µ—Ä: {file_size} –±–∞–π—Ç)")
        
        if language == "auto" or language is None:
            result = model.transcribe(file_path, fp16=False if model.device == "cpu" else True)
        else:
            result = model.transcribe(file_path, language=language, fp16=False if model.device == "cpu" else True)
        
        transcription = result["text"].strip()
        print(f"‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –≥–æ—Ç–æ–≤–∞ (–¥–ª–∏–Ω–∞: {len(transcription)} —Å–∏–º–≤–æ–ª–æ–≤)")
        return transcription
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {e}")
        raise e
